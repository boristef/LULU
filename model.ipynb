{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "def convert_to_chinese(number):\n",
    "    chinese_numerals = {\n",
    "        \"0\": \"零\",\n",
    "        \"1\": \"一\",\n",
    "        \"2\": \"二\",\n",
    "        \"3\": \"三\",\n",
    "        \"4\": \"四\",\n",
    "        \"5\": \"五\",\n",
    "        \"6\": \"六\",\n",
    "        \"7\": \"七\",\n",
    "        \"8\": \"八\",\n",
    "        \"9\": \"九\",\n",
    "    }\n",
    "    chinese_number = \"\".join([chinese_numerals[str(digit)] for digit in str(number)])\n",
    "    return chinese_number\n",
    "\n",
    "# 定義將文本中的阿拉伯數字轉換為中文數字的函數\n",
    "def convert_numbers_to_chinese(text):\n",
    "    # 確保text是字符串，如果不是，轉換為字符串\n",
    "    text = str(text)\n",
    "    words = jieba.lcut(text)\n",
    "    converted_words = ''.join([convert_to_chinese(word) if word.isdigit() else word for word in words])\n",
    "    return converted_words\n",
    "\n",
    "with open('./jieba/stop_words.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "# Remove empty strings and add additional stopwords if needed\n",
    "stopwords = [word for word in stopwords if word.strip()]\n",
    "stopwords.extend([' ', 'other_stopword'])\n",
    "\n",
    "# 載入第一個程式產生的檔案\n",
    "with open(\"tokenizer.pickle\", \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "word2vec_model = Word2Vec.load(\"word2vec.model\")\n",
    "embedding_dim = word2vec_model.vector_size  # 取得Word2Vec模型的維度\n",
    "vocab_list = [word for word,i in word2vec_model.wv.key_to_index.items()]  # 取得\n",
    "word_index = {\" \":0}\n",
    "word_vector = {}\n",
    "\n",
    "\n",
    "# 將文本轉換成詞向量\n",
    "embedding_matrix = np.zeros((len(vocab_list) + 1, embedding_dim))\n",
    "for i in range(len(vocab_list)):\n",
    "    word = vocab_list[i]\n",
    "    word_index[word] = i + 1\n",
    "    word_vector[word] = word2vec_model.wv[word]\n",
    "    embedding_matrix[i + 1] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "from collections import Counter\n",
    "\n",
    "data = pd.read_excel('train_data.xlsx')\n",
    "\n",
    "# 將 \"輸入\" 欄位中的阿拉伯數字轉換為中文數字\n",
    "data[\"輸入\"] = data[\"輸入\"].apply(convert_numbers_to_chinese)\n",
    "\n",
    "# 資料預處理（假設已有資料X和標籤y）\n",
    "X = []\n",
    "y = []\n",
    "z = []\n",
    "for i in range(len(data)):\n",
    "    words = jieba.lcut(data[\"輸入\"].iloc[i])\n",
    "    tmp = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            tmp.append(word)\n",
    "    X.append(tmp)\n",
    "    y.append(data[\"分類\"].iloc[i])\n",
    "    z.append(jieba.analyse.extract_tags(data['輸入'].iloc[i], topK=4))\n",
    "\n",
    "# Convert 'y' to categorical\n",
    "y_cate = to_categorical(np.asarray(y))\n",
    "\n",
    "# Calculate and display the count of each category\n",
    "category_counts = Counter(y)\n",
    "amount = sum(category_counts.values())\n",
    "total_samples = len(y)\n",
    "\n",
    "# Calculate and display the percentage of each category\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"Category {category}: {count} occurrences, Percentage: {percentage:.2f}%\")\n",
    "\n",
    "print(f\"Amount of categories: {amount}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text, word_index):\n",
    "    data = []\n",
    "    for sentence in text:\n",
    "        new_text = []\n",
    "        for word in sentence:\n",
    "            if word in word_index:\n",
    "                try:\n",
    "                    new_text.append(word_index[word])\n",
    "                except:\n",
    "                    new_text.append(0)\n",
    "        data.append(new_text)\n",
    "    data_lengths = [len(sentence) for sentence in data]\n",
    "    max_length = max(data_lengths)\n",
    "    \n",
    "    texts = pad_sequences(data, maxlen=7)\n",
    "    return texts\n",
    "with open('word_index.pkl', 'wb') as f:\n",
    "    pickle.dump(word_index, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切割訓練集和測試集\n",
    "use_data_version = [X,z]# X為斷詞結果，z為前4個關鍵字\n",
    "X = tokenizer(use_data_version[0], word_index)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_cate, test_size=0.2, random_state=42)\n",
    "# word_index\n",
    "# X_train = X\n",
    "# y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假設X是你的文本數據，已經通過tokenizer轉換為數字序列\n",
    "\n",
    "# 計算每個句子的長度\n",
    "sentence_lengths = [len(sentence) for sentence in X]\n",
    "\n",
    "# 計算平均數\n",
    "average_length = sum(sentence_lengths) / len(sentence_lengths)\n",
    "\n",
    "# 計算中位數\n",
    "sorted_lengths = sorted(sentence_lengths)\n",
    "mid = len(sorted_lengths) // 2\n",
    "median_length = (sorted_lengths[mid] + sorted_lengths[~mid]) / 2  # 使用~mid取得中位數，即使數量為奇數也可正確計算\n",
    "\n",
    "# 打印結果\n",
    "print(f\"平均數: {average_length}\")\n",
    "print(f\"中位數: {median_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, save_model\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, BatchNormalization, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.constraints import max_norm\n",
    "from keras.initializers import he_normal\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from datetime import datetime\n",
    "# 創建模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(embedding_matrix), output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix], input_length=X.shape[1], \n",
    "                    trainable=False, embeddings_initializer=he_normal()))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, \n",
    "                             kernel_constraint=max_norm(5), kernel_regularizer=l2(0.01))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(32, kernel_constraint=max_norm(5), kernel_regularizer=l2(0.01))))\n",
    "model.add(Dropout(0.3))  # 添加 Dropout 層，可根據需要調整 dropout 比例\n",
    "model.add(Dense(6, activation='softmax', kernel_regularizer=l2(0.01)))  # 在全連接層上應用 L2 正規化\n",
    "\n",
    "# 編譯模型，使用學習率調整器\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.005), metrics=['accuracy'])\n",
    "\n",
    "# 訓練模型，加入 callbacks\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[reduce_lr])\n",
    "\n",
    "# Generate today's date in the format YYYYMMDD\n",
    "today_date = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "# Construct the filename using the specified naming convention\n",
    "filename = f'LSTMmodel_{today_date}.keras'\n",
    "\n",
    "# Save the entire model in the .keras format with the constructed filename\n",
    "# save_model(model, filename)\n",
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練(bigru early stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import class_weight\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential, save_model\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "import numpy as np\n",
    "from keras.regularizers import l2\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the Sequential model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(embedding_matrix), output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix], input_length=X.shape[1], \n",
    "                    trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=50, return_sequences=True, kernel_regularizer=l2(0.01))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(units=50, return_sequences=True, kernel_regularizer=l2(0.01))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(units=50, return_sequences=True, kernel_regularizer=l2(0.01))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(units=50, return_sequences=True, kernel_regularizer=l2(0.01))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(units=50, return_sequences=True, kernel_regularizer=l2(0.01))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(units=50, kernel_regularizer=l2(0.01))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and Adam optimizer with specified learning rate\n",
    "# Also, include accuracy as a metric\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Add EarlyStopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the training data and validate on the test data\n",
    "# Include the ReduceLROnPlateau and EarlyStopping callbacks\n",
    "# model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[reduce_lr, early_stop])\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Train the model with class weights\n",
    "history = model.fit(X, y_cate, epochs=80, batch_size=64, validation_split=0.2, class_weight=class_weights_dict, callbacks=[reduce_lr,early_stop])\n",
    "\n",
    "# Generate today's date in the format YYYYMMDD\n",
    "today_date = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "# Construct the filename using the specified naming convention\n",
    "filename = f'BiLSTMmodel_{today_date}.keras'\n",
    "\n",
    "# Save the entire model in the .keras format with the constructed filename\n",
    "save_model(model, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten, LSTM\n",
    "\n",
    "from keras.layers import Dropout\n",
    "Embedding(input_dim=len(embedding_matrix), output_dim=embedding_dim, \n",
    "                                weights=[embedding_matrix], input_length=X.shape[1], \n",
    "                                trainable=False)\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(embedding_matrix), output_dim=embedding_dim, \n",
    "                                weights=[embedding_matrix], input_length=X.shape[1], \n",
    "                                trainable=False))\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(LSTM(units=50, return_sequences=True))\n",
    "model.add(LSTM(units=50, return_sequences=False))  # 新增一層 LSTM\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# 打印模型摘要\n",
    "model.summary()\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, MultiHeadAttention, Embedding, Flatten, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from keras.layers import concatenate\n",
    "\n",
    "# 修改 create_model_with_self_attention 函數\n",
    "def create_model_with_attention_and_lstm(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']):\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "    \n",
    "    # 添加嵌入層，使用預訓練的嵌入矩陣\n",
    "    embedding_layer = Embedding(input_dim=len(embedding_matrix), output_dim=embedding_dim, \n",
    "                                weights=[embedding_matrix], input_length=X.shape[1], \n",
    "                                trainable=False)(inputs)\n",
    "    \n",
    "    # 添加自注意力機制（self-attention）\n",
    "    attention = MultiHeadAttention(num_heads=1, key_dim=50)(embedding_layer, embedding_layer, embedding_layer)\n",
    "    \n",
    "    # Flatten 注意力的輸出\n",
    "    attention_flatten = Flatten()(attention)\n",
    "    \n",
    "    # LSTM 層\n",
    "    lstm_layer = LSTM(64)(embedding_layer)\n",
    "    \n",
    "    # 將自注意力的輸出和LSTM的輸出連接\n",
    "    merged_layer = concatenate([attention_flatten, lstm_layer])\n",
    "    \n",
    "    # 全連接層\n",
    "    dense_layer = Dense(64, activation='relu')(merged_layer)\n",
    "    batch_norm = BatchNormalization()(dense_layer)\n",
    "    dropout = Dropout(0.3)(batch_norm)\n",
    "    \n",
    "    # 輸出層，使用 softmax 激活函數\n",
    "    output_layer = Dense(6, activation='softmax')(dropout)\n",
    "    \n",
    "    # 創建模型\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "    \n",
    "    # 編譯模型\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 使用最佳學習率創建具有自注意力機制的模型\n",
    "model_with_self_attention = create_model_with_attention_and_lstm()\n",
    "\n",
    "# 生成當天日期的字符串\n",
    "today_date = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "# 構建文件名，使用指定的命名慣例\n",
    "filename = f'NNmodel_{today_date}.tf'\n",
    "\n",
    "# 建立 ModelCheckpoint 回調以保存最佳模型\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "model_with_self_attention.summary()\n",
    "# 模型訓練\n",
    "model_with_self_attention.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練(textcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import keras\n",
    "from keras.layers import concatenate, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Dense, Input\n",
    "from keras.models import Model, save_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.initializers import he_normal\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from datetime import datetime\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "# 生成當天日期的字符串\n",
    "today_date = datetime.today().strftime('%Y%m%d')\n",
    "#数据预处理\n",
    "def data_process(path, max_len=50):           #path为句子的存储路径，max_len为句子的固定长度\n",
    "    dataset = pd.read_excel(path,  names=['輸入', '分類']).astype(str)\n",
    "    cw = lambda x: list(jieba.cut(x))         # 定义分词函数\n",
    "    dataset['words'] = dataset['輸入'].apply(cw)  # 将句子进行分词\n",
    "    tokenizer = Tokenizer()                   # 创建一个Tokenizer对象，将一个词转换为正整数\n",
    "    tokenizer.fit_on_texts(dataset['words'])  #将词编号，词频越大，编号越小\n",
    "    vocab = tokenizer.word_index              # 得到每个词的编号\n",
    "    x_train, x_test, y_train, y_test = train_test_split(dataset['words'], dataset['分類'], test_size=0.1)  #划分数据集\n",
    "    x_train_word_ids = tokenizer.texts_to_sequences(x_train)     #将测试集列表中每个词转换为数字\n",
    "    x_test_word_ids = tokenizer.texts_to_sequences(x_test)       #将训练集列表中每个词转换为数字\n",
    "    x_train_padded_seqs = pad_sequences(x_train_word_ids, maxlen=max_len)  # 将每个句子设置为等长，每句默认为50\n",
    "    x_test_padded_seqs = pad_sequences(x_test_word_ids, maxlen=max_len)    #将超过固定值的部分截掉，不足的在最前面用0填充\n",
    "    return x_train_padded_seqs,y_train,x_test_padded_seqs,y_test,vocab\n",
    "# 构建TextCNN模型\n",
    "def TextCNN_model_1(x_train, y_train, x_test, y_test):\n",
    "    main_input = Input(shape=(7,), dtype='float64')\n",
    "    # 嵌入层（使用预训练的词向量）\n",
    "    embedder = Embedding(input_dim=len(embedding_matrix), output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix], input_length=X.shape[1], \n",
    "                    trainable=False, embeddings_initializer=he_normal())\n",
    "    embed = embedder(main_input)\n",
    "    # 卷积层和池化层，设置卷积核大小分别为3,4,5\n",
    "    cnn1 = Conv1D(embedding_dim, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn1 = MaxPooling1D(pool_size=6)(cnn1)\n",
    "    cnn2 = Conv1D(embedding_dim, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn2 = MaxPooling1D(pool_size=5)(cnn2)\n",
    "    cnn3 = Conv1D(embedding_dim, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn3 = MaxPooling1D(pool_size=4)(cnn3)\n",
    "    # 合并三个模型的输出向量\n",
    "    cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "    flat = Flatten()(cnn)\n",
    "    drop = Dropout(0.2)(flat) #在池化层到全连接层之前可以加上dropout防止过拟合\n",
    "    main_output = Dense(6, activation='softmax')(drop)\n",
    "    model = Model(inputs=main_input, outputs=main_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # 構建文件名，使用指定的命名慣例\n",
    "    filename = f'textCNNmodel_{today_date}.keras'\n",
    "    # one_hot_labels = keras.utils.to_categorical(y_train, num_classes=3)  # 将标签转换为one-hot编码\n",
    "    # Add EarlyStopping callback\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint(filename, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "    history = model.fit(x_train, y_train, batch_size=64, validation_data=(X_test, y_test), epochs=120, callbacks=[early_stop])\n",
    "    result = model.predict(x_test)  # 预测样本属于每个类别的概率\n",
    "    result_labels = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "    y_predict = list(map(int, result_labels))\n",
    "    y=np.argmax(y_test, axis=-1)\n",
    "    print(y_predict)\n",
    "    print('准确率', metrics.accuracy_score(y, y_predict))\n",
    "    # Save the entire model in the .keras format with the constructed filename\n",
    "    save_model(model, filename)\n",
    "    return history\n",
    "# path = 'train_data.xlsx'\n",
    "# x_train, y_train, x_test, y_test, vocab = data_process(path)\n",
    "history = TextCNN_model_1(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練 Ernie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import jieba\n",
    "import jieba.analyse\n",
    "data = pd.read_excel('train_data.xlsx')\n",
    "with open('./jieba/stop_words.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "# Remove empty strings and add additional stopwords if needed\n",
    "stopwords = [word for word in stopwords if word.strip()]\n",
    "stopwords.extend([' ', 'other_stopword'])\n",
    "\n",
    "def convert_to_chinese(number):\n",
    "    chinese_numerals = {\n",
    "        \"0\": \"零\",\n",
    "        \"1\": \"一\",\n",
    "        \"2\": \"二\",\n",
    "        \"3\": \"三\",\n",
    "        \"4\": \"四\",\n",
    "        \"5\": \"五\",\n",
    "        \"6\": \"六\",\n",
    "        \"7\": \"七\",\n",
    "        \"8\": \"八\",\n",
    "        \"9\": \"九\",\n",
    "    }\n",
    "    chinese_number = \"\".join([chinese_numerals[str(digit)] for digit in str(number)])\n",
    "    return chinese_number\n",
    "\n",
    "# 定義將文本中的阿拉伯數字轉換為中文數字的函數\n",
    "def convert_numbers_to_chinese(text):\n",
    "    # 確保text是字符串，如果不是，轉換為字符串\n",
    "    text = str(text)\n",
    "    converted_words = ''.join([convert_to_chinese(word) if word.isdigit() else word for word in text]).replace('，', '').replace(',', '')\n",
    "    return converted_words\n",
    "# 將 \"輸入\" 欄位中的阿拉伯數字轉換為中文數字\n",
    "print(data[\"輸入\"])\n",
    "data[\"輸入\"] = data[\"輸入\"].apply(convert_numbers_to_chinese)\n",
    "print(data[\"輸入\"])\n",
    "# 資料預處理（假設已有資料X和標籤y）\n",
    "X = []\n",
    "y = []\n",
    "z = []\n",
    "for i in range(len(data)):\n",
    "    words = jieba.lcut(data[\"輸入\"].iloc[i])\n",
    "    tmp=[]\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            tmp.append(word)\n",
    "    X.append(tmp)\n",
    "    y.append(data[\"分類\"].iloc[i])\n",
    "    z.append(jieba.analyse.extract_tags(data['輸入'].iloc[i], topK=4) )\n",
    "# y_cate = to_categorical(np.asarray(y))\n",
    "print(X)\n",
    "print(y)\n",
    "print(z)\n",
    "formatted_data = list(zip([' '.join(sentence) for sentence in X], y))\n",
    "maxlen = max(len(seq) for seq in X)\n",
    "# df = pd.DataFrame({'Text':X,'Label':y})\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ernie import SentenceClassifier, Models\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(formatted_data)\n",
    "\n",
    "classifier = SentenceClassifier(\n",
    "    model_name=(Models.BertBaseUncased),\n",
    "    max_length=12,\n",
    "    labels_no=6,\n",
    "    \n",
    ")\n",
    "batchsize = 2\n",
    "# Load dataset and fine-tune the classifier\n",
    "classifier.load_dataset(df )\n",
    "history = classifier.fine_tune(\n",
    "    epochs=30,\n",
    "    learning_rate=2e-5,\n",
    "    training_batch_size=batchsize,\n",
    "    validation_batch_size=batchsize,\n",
    ")\n",
    "# classifier.dump('./ernie3.model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試(手動輸入)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boris/miniconda3/envs/ernie/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-25 10:58:13.094217: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-25 10:58:13.164940: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-25 10:58:13.182303: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-25 10:58:13.457060: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:\n",
      "2024-01-25 10:58:13.457130: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:\n",
      "2024-01-25 10:58:13.457135: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-01-25 10:58:15.242030: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-25 10:58:15.348322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.361850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.362100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.500526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.500659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.500665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-25 10:58:15.500762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.500792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 13526 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-01-25 10:58:15.540060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.540303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.540460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.540831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.541017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.541352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.541697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.541722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-25 10:58:15.541931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-25 10:58:15.541961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13526 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-01-25 10:58:15.914425: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at ./ernie2.model were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ./ernie2.model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Predicted Class: 0重新輸入\n",
      "Second predicted Class: 5其他相關\n",
      "Predictions: [(0.5343225528578964, 0.07113374233913006, 0.1349709062684465, 0.037900927354390365, 0.06373803362929155, 0.15793383755084509)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2024-01-25 10:58:30 [DEBUG] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2024-01-25 10:58:30 [DEBUG] Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "貨件到哪裡了\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.225 seconds.\n",
      "2024-01-25 10:58:31 [DEBUG] Loading model cost 0.225 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2024-01-25 10:58:31 [DEBUG] Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "貨件 到 哪裡 了\n",
      "Predicted Class: 3客訴\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.012552255736375473, 0.20234490148319612, 0.2530043374383476, 0.29987032886306914, 0.01817702587726872, 0.2140511506017428)]\n",
      "貨件到哪裡了\n",
      "貨件 到 哪裡 了\n",
      "Predicted Class: 3客訴\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.012552255736375473, 0.20234490148319612, 0.2530043374383476, 0.29987032886306914, 0.01817702587726872, 0.2140511506017428)]\n",
      "貨件到哪裡了\n",
      "貨件 到 哪裡 了\n",
      "Predicted Class: 3客訴\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.012552255736375473, 0.20234490148319612, 0.2530043374383476, 0.29987032886306914, 0.01817702587726872, 0.2140511506017428)]\n",
      "貨件到哪裡了\n",
      "貨件 到 哪裡 了\n",
      "Predicted Class: 3客訴\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.012552255736375473, 0.20234490148319612, 0.2530043374383476, 0.29987032886306914, 0.01817702587726872, 0.2140511506017428)]\n",
      "我的貨件到哪裡了\n",
      "我 的 貨件 到 哪裡 了\n",
      "Predicted Class: 1查件\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.0011809063291520927, 0.9853689525646834, 0.009408288711518676, 0.0009898197666282384, 0.0017758958157671893, 0.0012761368122505376)]\n",
      "我的貨件到哪裡了\n",
      "我 的 貨件 到 哪裡 了\n",
      "Predicted Class: 1查件\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.0011809063291520927, 0.9853689525646834, 0.009408288711518676, 0.0009898197666282384, 0.0017758958157671893, 0.0012761368122505376)]\n",
      "貨號一二三四五到哪裡了\n",
      "貨號 一 二三四五到 哪裡 了\n",
      "Predicted Class: 0重新輸入\n",
      "Second predicted Class: 1查件\n",
      "Predictions: [(0.8178745729709784, 0.08368125768717466, 0.00696061701983385, 0.060737896234333184, 0.02256130281036034, 0.008184353277319527)]\n",
      "貨號一二三四五到哪裡了\n",
      "貨號 一 二三四五到 哪裡 了\n",
      "Predicted Class: 0重新輸入\n",
      "Second predicted Class: 1查件\n",
      "Predictions: [(0.8178745729709784, 0.08368125768717466, 0.00696061701983385, 0.060737896234333184, 0.02256130281036034, 0.008184353277319527)]\n",
      "零零零零零零零零零零零\n",
      "零零零零零零零零零零零\n",
      "Predicted Class: 5其他相關\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.013781310595292116, 0.148613154307013, 0.25170500568175463, 0.21426231436929555, 0.0124146527560559, 0.3592235622905888)]\n",
      "零零零零零零零零零零\n",
      "零零零零零零零零零零\n",
      "Predicted Class: 5其他相關\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.013518467113666098, 0.16484334640406068, 0.2529155182376347, 0.23943840193801508, 0.014010988060430295, 0.31527327824619317)]\n",
      "貨號零零零零零零零零零零\n",
      "貨號 零零零零零零零零零零\n",
      "Predicted Class: 5其他相關\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.013683957776487302, 0.14461431793592205, 0.2555796827875972, 0.20459521978825046, 0.011906855558248492, 0.36961996615349446)]\n",
      "貨號零零零零零零零零零零\n",
      "貨號 零零零零零零零零零零\n",
      "Predicted Class: 5其他相關\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.013683957776487302, 0.14461431793592205, 0.2555796827875972, 0.20459521978825046, 0.011906855558248492, 0.36961996615349446)]\n",
      "貨號零零零零零零零零零零零\n",
      "貨號 零零零零零零零零零零零\n",
      "Predicted Class: 5其他相關\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.01408848344264225, 0.13671375491347873, 0.2566756116154197, 0.19541205695816288, 0.011211374528923327, 0.3858987185413731)]\n",
      "貨號零零零零零零零零零零零\n",
      "貨號 零零零零零零零零零零零\n",
      "Predicted Class: 5其他相關\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.01408848344264225, 0.13671375491347873, 0.2566756116154197, 0.19541205695816288, 0.011211374528923327, 0.3858987185413731)]\n",
      "我的貨件預計在什麼時候抵達？\n",
      "我 的 貨件 預計 在 什麼 時候 抵達 ？\n",
      "Predicted Class: 1查件\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.0013886899174059572, 0.9546461635483028, 0.03901198495194226, 0.0006939095457510561, 0.0017757441699314893, 0.002483507866666476)]\n",
      "我的貨件預計在什麼時候抵達？\n",
      "我 的 貨件 預計 在 什麼 時候 抵達 ？\n",
      "Predicted Class: 1查件\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.0013886899174059572, 0.9546461635483028, 0.03901198495194226, 0.0006939095457510561, 0.0017757441699314893, 0.002483507866666476)]\n",
      "我的貨件預計在什麼時候抵達？ 可以了解他現在的貨況嗎\n",
      "我 的 貨件 預計 在 什麼 時候 抵達 ？   可以 了解 他 現在 的 貨況 嗎\n",
      "Predicted Class: 1查件\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.0007766189078768971, 0.978916116069958, 0.014844890818752648, 0.001557038990727248, 0.002223168305823401, 0.0016821669068618587)]\n",
      "\n",
      "\n",
      "Predicted Class: 0重新輸入\n",
      "Second predicted Class: 5其他相關\n",
      "Predictions: [(0.5343225528578964, 0.07113374233913006, 0.1349709062684465, 0.037900927354390365, 0.06373803362929155, 0.15793383755084509)]\n",
      "氣死我了,我的貨件預計在什麼時候抵達？\n",
      "氣死 我 了 , 我 的 貨件 預計 在 什麼 時候 抵達 ？\n",
      "Predicted Class: 1查件\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.0009653198993083951, 0.980876036121326, 0.014261801237158648, 0.0005090672263510493, 0.0016100660448117043, 0.0017777094710442188)]\n",
      "\n",
      "\n",
      "Predicted Class: 0重新輸入\n",
      "Second predicted Class: 5其他相關\n",
      "Predictions: [(0.5343225528578964, 0.07113374233913006, 0.1349709062684465, 0.037900927354390365, 0.06373803362929155, 0.15793383755084509)]\n",
      "氣死我了,我的貨件預計在什麼時候抵達？\n",
      "氣死 我 了 , 我 的 貨件 預計 在 什麼 時候 抵達 ？\n",
      "Predicted Class: 1查件\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.0009653198993083951, 0.980876036121326, 0.014261801237158648, 0.0005090672263510493, 0.0016100660448117043, 0.0017777094710442188)]\n",
      "氣死我了,我的貨件預計在什麼時候抵達？\n",
      "氣死 我 了 , 我 的 貨件 預計 在 什麼 時候 抵達 ？\n",
      "Predicted Class: 1查件\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.0009653198993083951, 0.980876036121326, 0.014261801237158648, 0.0005090672263510493, 0.0016100660448117043, 0.0017777094710442188)]\n",
      "氣死我了,我的貨件預計在什麼時候抵達？\n",
      "氣死 我 了 , 我 的 貨件 預計 在 什麼 時候 抵達 ？\n",
      "Predicted Class: 1查件\n",
      "Second predicted Class: 2營業所\n",
      "Predictions: [(0.0009653198993083951, 0.980876036121326, 0.014261801237158648, 0.0005090672263510493, 0.0016100660448117043, 0.0017777094710442188)]\n",
      "氣死我了,我的貨件預計在什麼時候抵達？一二三\n",
      "氣死 我 了 , 我 的 貨件 預計 在 什麼 時候 抵達 ？ 一 二三\n",
      "Predicted Class: 1查件\n",
      "Second predicted Class: 4服務時間\n",
      "Predictions: [(0.02462899078029683, 0.9240558765320487, 0.007877618973463526, 0.0054242509663071274, 0.034496020205218886, 0.0035172425426648357)]\n",
      "\n",
      "\n",
      "Predicted Class: 0重新輸入\n",
      "Second predicted Class: 5其他相關\n",
      "Predictions: [(0.5343225528578964, 0.07113374233913006, 0.1349709062684465, 0.037900927354390365, 0.06373803362929155, 0.15793383755084509)]\n",
      "氣死 我 了 , 我 的 貨件 預計 在 什麼 時候 抵達 ？ 一 二三\n",
      "氣死   我   了   ,   我   的   貨件   預計   在   什麼   時候   抵達   ？   一   二三\n",
      "Predicted Class: 1查件\n",
      "Second predicted Class: 4服務時間\n",
      "Predictions: [(0.02462899078029683, 0.9240558765320487, 0.007877618973463526, 0.0054242509663071274, 0.034496020205218886, 0.0035172425426648357)]\n",
      "\n",
      "\n",
      "Predicted Class: 0重新輸入\n",
      "Second predicted Class: 5其他相關\n",
      "Predictions: [(0.5343225528578964, 0.07113374233913006, 0.1349709062684465, 0.037900927354390365, 0.06373803362929155, 0.15793383755084509)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# for _ in range(1):\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# 假設 user_input 是用戶輸入的文本\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m請輸入文本：\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# 將數字轉換為中文\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     49\u001b[0m         [convert_to_chinese(ch) \u001b[38;5;28;01mif\u001b[39;00m ch\u001b[38;5;241m.\u001b[39misdigit() \u001b[38;5;28;01melse\u001b[39;00m ch \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(user_input)]\n\u001b[1;32m     50\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ernie/lib/python3.9/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ernie/lib/python3.9/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from ernie import SentenceClassifier\n",
    "import jieba\n",
    "\n",
    "classifier = SentenceClassifier(model_path='./ernie2.model')\n",
    "cate = {0: '重新輸入', 1: '查件', 2: '營業所', 3: '客訴', 4: '服務時間', 5: '其他相關'}\n",
    "\n",
    "def chinese_text_segmentation(text):\n",
    "    seg_list = jieba.cut(text)\n",
    "    return \" \".join(seg_list)\n",
    "\n",
    "def convert_to_chinese(number):\n",
    "    chinese_numerals = {\n",
    "        \"0\": \"零\",\n",
    "        \"1\": \"一\",\n",
    "        \"2\": \"二\",\n",
    "        \"3\": \"三\",\n",
    "        \"4\": \"四\",\n",
    "        \"5\": \"五\",\n",
    "        \"6\": \"六\",\n",
    "        \"7\": \"七\",\n",
    "        \"8\": \"八\",\n",
    "        \"9\": \"九\",\n",
    "    }\n",
    "    chinese_number = \"\".join([chinese_numerals[digit] for digit in str(number)])\n",
    "    return chinese_number\n",
    "\n",
    "def tokenizer(text, word_index):\n",
    "    data = []\n",
    "    for sentence in text:\n",
    "        new_text = []\n",
    "        for word in sentence:\n",
    "            if word in word_index:\n",
    "                try:\n",
    "                    new_text.append(word_index[word])\n",
    "                except:\n",
    "                    new_text.append(0)\n",
    "        data.append(new_text)\n",
    "    return data\n",
    "\n",
    "# for _ in range(1):\n",
    "while True:\n",
    "    # 假設 user_input 是用戶輸入的文本\n",
    "    user_input = input(\"請輸入文本：\")\n",
    "\n",
    "    # 將數字轉換為中文\n",
    "    user_input = \"\".join(\n",
    "        [convert_to_chinese(ch) if ch.isdigit() else ch for ch in str(user_input)]\n",
    "    )\n",
    "    print(user_input)\n",
    "\n",
    "    # 讀取 word_index\n",
    "    with open('word_index.pkl', 'rb') as f:\n",
    "        word_index = pickle.load(f)\n",
    "\n",
    "    # 使用 jieba 進行斷詞\n",
    "    user_input_segmented = chinese_text_segmentation(user_input)\n",
    "\n",
    "    # 使用 SentenceClassifier 進行預測\n",
    "    print(user_input_segmented)\n",
    "    predictions = list(classifier.predict([user_input_segmented]))\n",
    "    predicted_class = np.array(predictions).argmax(axis=-1)[0]\n",
    "    predicted_class2 = np.argsort(predictions[0])[-2]\n",
    "    # 打印預測結果\n",
    "    print(f\"Predicted Class: {str(predicted_class) + cate[predicted_class]}\")\n",
    "    print(f\"Second predicted Class: {str(predicted_class2) + cate[predicted_class2]}\")\n",
    "    print(f\"Predictions: {predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試(test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import jieba\n",
    "data = pd.read_excel('train_data.xlsx',sheet_name='test')\n",
    "def convert_to_chinese(number):\n",
    "    chinese_numerals = {\n",
    "        \"0\": \"零\",\n",
    "        \"1\": \"一\",\n",
    "        \"2\": \"二\",\n",
    "        \"3\": \"三\",\n",
    "        \"4\": \"四\",\n",
    "        \"5\": \"五\",\n",
    "        \"6\": \"六\",\n",
    "        \"7\": \"七\",\n",
    "        \"8\": \"八\",\n",
    "        \"9\": \"九\",\n",
    "    }\n",
    "    chinese_number = \"\".join([chinese_numerals[str(digit)] for digit in str(number)])\n",
    "    return chinese_number\n",
    "\n",
    "# 定義將文本中的阿拉伯數字轉換為中文數字的函數\n",
    "def convert_numbers_to_chinese(text):\n",
    "    # 確保text是字符串，如果不是，轉換為字符串\n",
    "    text = str(text)\n",
    "    words = jieba.lcut(text)\n",
    "    converted_words = ''.join([convert_to_chinese(word) if word.isdigit() else word for word in words])\n",
    "    return converted_words\n",
    "# 將 \"輸入\" 欄位中的阿拉伯數字轉換為中文數字\n",
    "data[\"輸入\"] = data[\"輸入\"].apply(convert_numbers_to_chinese)\n",
    "# 資料預處理（假設已有資料X和標籤y）\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(data)):\n",
    "    X.append(\" \".join(jieba.lcut(data[\"輸入\"].iloc[i])))\n",
    "    y.append(data[\"分類\"].iloc[i])\n",
    "# y_cate = to_categorical(np.asarray(y))\n",
    "print(X)\n",
    "print(y)\n",
    "# formatted_data = list(zip([' '.join(sentence) for sentence in X], y))\n",
    "df = pd.DataFrame({'Text':X,'Label':y})\n",
    "# df['Text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ernie import SentenceClassifier\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# classifier = SentenceClassifier(model_path='./ernie-autosave/bert/1705565624717/')\n",
    "classifier = SentenceClassifier(model_path='./ernie-autosave/bert/1705891874320/')\n",
    "cate={0:'重新輸入',1:'查件',2:'營業所',3:'客訴',4:'服務時間',5:'其他相關'}\n",
    "\n",
    "# 預測結果\n",
    "predictions = list(classifier.predict(df['Text'].tolist()))\n",
    "# 獲取最大概率的類別\n",
    "predicted_class = np.array(predictions).argmax(axis=-1)\n",
    "df['predict']=predicted_class\n",
    "# 打印預測結果\n",
    "matching_rows = df[df['Label'] == df['predict']]\n",
    "\n",
    "# 計算百分比\n",
    "percentage = (len(matching_rows) / len(df)) * 100\n",
    "\n",
    "print(f\"Label等於Predict的百分比為: {percentage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到Label不等於Predict的行\n",
    "non_matching_rows = df[df['Label'] != df['predict']]\n",
    "\n",
    "# 顯示不匹配的行\n",
    "print(\"不匹配的行:\")\n",
    "print(non_matching_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ernie 2 (transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, ErnieForSequenceClassification, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# 載入預先訓練好的 BERT 模型和分詞器\n",
    "model_name = 'maidalun1020/bce-reranker-base_v1'\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertForSequenceClassification.from_pretrained(model_name, num_labels=6)  # 設置 num_labels 為 6\n",
    "# model = ErnieForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "# 假設你有一個包含文本和標籤的資料集\n",
    "# 記得將文本轉換成 BERT 模型可接受的格式（使用分詞器）\n",
    "data = pd.read_excel('train_data.xlsx', dtype={'輸入': str, '分類': int})\n",
    "texts = data['輸入'].tolist()  # 文本轉換成列表\n",
    "labels = data['分類'].tolist()  # 標籤（0 到 5）\n",
    "print(texts)\n",
    "print(labels)\n",
    "# 將文本轉換成 BERT 模型的輸入格式\n",
    "encoded_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# 建立 PyTorch Dataset\n",
    "dataset = TensorDataset(encoded_texts['input_ids'], encoded_texts['attention_mask'], torch.tensor(labels))\n",
    "# 將資料集分為訓練集和驗證集\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "batch_size = 4\n",
    "# 使用 DataLoader 加載資料\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定義優化器和損失函數\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # 使用 CrossEntropyLoss 替代 BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練模型\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Inside the training loop\n",
    "    model.train()\n",
    "    # 使用 tqdm 來顯示進度條\n",
    "    train_dataloader = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}', dynamic_ncols=True)\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # print(labels)\n",
    "        labels = labels.long()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # 在驗證集上評估模型\n",
    "    # Inside the validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "            _, predicted_labels = torch.max(outputs.logits, 1)\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# 儲存模型的參數\n",
    "model_name = model_name.split('/')[-1]\n",
    "torch.save(model.state_dict(), f'./models/{model_name}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試參數用: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming model, train_dataset, val_dataset, optimizer initialization elsewhere\n",
    "\n",
    "# Defining a list of epochs and learning rates to try\n",
    "epochs_list = [3, 5, 8]\n",
    "learning_rates = [5e-5, 0.001, 0.01, 0.1]\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_loss = 1.0\n",
    "best_epochs_loss = 0\n",
    "best_epochs = 0\n",
    "best_lr = 0.0\n",
    "\n",
    "# Iterating through different learning rates\n",
    "for lr in learning_rates:\n",
    "    print(f\"Testing learning rate: {lr}\")\n",
    "    \n",
    "    # Initializing optimizer with the current learning rate\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # Iterating through different epochs\n",
    "    for epochs in epochs_list:\n",
    "        # Training loop for each epoch\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "\n",
    "            train_dataloader = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}', dynamic_ncols=True)\n",
    "\n",
    "            for batch in train_dataloader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                optimizer.zero_grad()\n",
    "                labels = labels.long()\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dataloader:\n",
    "                    input_ids, attention_mask, labels = batch\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    val_loss += outputs.loss.item()\n",
    "                    _, predicted_labels = torch.max(outputs.logits, 1)\n",
    "                    correct_predictions += (predicted_labels == labels).sum().item()\n",
    "                    total_samples += labels.size(0)\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_dataloader)\n",
    "            accuracy = correct_predictions / total_samples\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "        # Checking if the current model is the best based on accuracy or loss\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_epochs = epochs\n",
    "            best_lr = lr\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            best_epochs_loss = epochs\n",
    "\n",
    "# Printing the best parameters\n",
    "print(f'Best number of epochs: {best_epochs}, Best accuracy: {best_accuracy:.4f}')\n",
    "print(f'Best number of epochs_loss: {best_epochs_loss}, Best loss: {best_loss:.4f}')\n",
    "print(f'Best learning rate: {best_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試transformer(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 創建 DataFrame\n",
    "df = pd.DataFrame({'Text': X, 'Label': y})\n",
    "model_name = 'bert-base-chinese'\n",
    "# 載入保存的 BERT 模型\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 讀取保存的模型參數\n",
    "model.load_state_dict(torch.load(f'./models/{model_name}'))\n",
    "\n",
    "# 設置模型為評估模式\n",
    "model.eval()\n",
    "\n",
    "# 將文本轉換為 BERT 模型的輸入格式\n",
    "encoded_texts = tokenizer(df['Text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# 進行預測\n",
    "with torch.no_grad():\n",
    "    outputs = model(encoded_texts['input_ids'], attention_mask=encoded_texts['attention_mask'])\n",
    "    _, predicted_labels = torch.max(outputs.logits, 1)\n",
    "\n",
    "# 將預測結果添加到 DataFrame\n",
    "df['Predicted_Label'] = predicted_labels.numpy()\n",
    "\n",
    "# 打印 DataFrame\n",
    "print(df)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 假設你已經有一個 DataFrame df 包含了 'Label' 和 'Predicted_Label' 列\n",
    "y_true = df['Label'].tolist()\n",
    "y_pred = df['Predicted_Label'].tolist()\n",
    "\n",
    "# 計算精確度\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "測試transformer(手動輸入)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "cate={0:'重新輸入',1:'查件',2:'營業所',3:'客訴',4:'服務時間',5:'其他相關'}\n",
    "# 載入保存的 BERT 模型\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=6)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "# 讀取保存的模型參數\n",
    "model.load_state_dict(torch.load('transformer.bert'))\n",
    "\n",
    "# 設置模型為評估模式\n",
    "model.eval()\n",
    "while True:\n",
    "    # 使用者手動輸入測試文本\n",
    "    user_input_text = input(\"請輸入測試文本: \")\n",
    "\n",
    "    # 將使用者輸入的文本轉換為 BERT 模型的輸入格式\n",
    "    encoded_text = tokenizer(user_input_text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # 進行預測\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_text)\n",
    "        _, predicted_label = torch.max(output.logits, 1)\n",
    "\n",
    "    # 打印預測結果\n",
    "    print(f'預測標籤: {predicted_label.item()} {cate[predicted_label.item()]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output figure loss & accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[0:, ['loss', 'val_loss']].plot()\n",
    "print((\"Minimum Training Loss: {:0.4f}\").format(history_df['loss'].min()))\n",
    "print((\"Minimum Validation Loss: {:0.4f}\").format(history_df['val_loss'].min()))\n",
    "history_df.loc[0:, ['accuracy', 'val_accuracy']].plot()\n",
    "print((\"Maximum Training Accuracy: {:0.4f}\").format(history_df['accuracy'].max()))\n",
    "print((\"Maximum Validation Accuracy: {:0.4f}\").format(history_df['val_accuracy'].max()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
