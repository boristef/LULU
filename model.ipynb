{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 11:38:00.244674: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-26 11:38:00.264620: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-26 11:38:00.264640: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-26 11:38:00.264654: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-26 11:38:00.269476: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "def convert_to_chinese(number):\n",
    "    chinese_numerals = {\n",
    "        \"0\": \"零\",\n",
    "        \"1\": \"一\",\n",
    "        \"2\": \"二\",\n",
    "        \"3\": \"三\",\n",
    "        \"4\": \"四\",\n",
    "        \"5\": \"五\",\n",
    "        \"6\": \"六\",\n",
    "        \"7\": \"七\",\n",
    "        \"8\": \"八\",\n",
    "        \"9\": \"九\",\n",
    "    }\n",
    "    chinese_number = \"\".join([chinese_numerals[str(digit)] for digit in str(number)])\n",
    "    return chinese_number\n",
    "\n",
    "# 定義將文本中的阿拉伯數字轉換為中文數字的函數\n",
    "def convert_numbers_to_chinese(text):\n",
    "    # 確保text是字符串，如果不是，轉換為字符串\n",
    "    text = str(text)\n",
    "    words = jieba.lcut(text)\n",
    "    converted_words = ''.join([convert_to_chinese(word) if word.isdigit() else word for word in words])\n",
    "    return converted_words\n",
    "\n",
    "\n",
    "\n",
    "# 載入第一個程式產生的檔案\n",
    "with open(\"tokenizer.pickle\", \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "word2vec_model = Word2Vec.load(\"word2vec.model\")\n",
    "embedding_dim = word2vec_model.vector_size  # 取得Word2Vec模型的維度\n",
    "vocab_list = [word for word,i in word2vec_model.wv.key_to_index.items()]  # 取得\n",
    "word_index = {\" \":0}\n",
    "word_vector = {}\n",
    "\n",
    "\n",
    "# 將文本轉換成詞向量\n",
    "embedding_matrix = np.zeros((len(vocab_list) + 1, embedding_dim))\n",
    "for i in range(len(vocab_list)):\n",
    "    word = vocab_list[i]\n",
    "    word_index[word] = i + 1\n",
    "    word_vector[word] = word2vec_model.wv[word]\n",
    "    embedding_matrix[i + 1] = word2vec_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.211 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('train_data.xlsx')\n",
    "\n",
    "# 將 \"輸入\" 欄位中的阿拉伯數字轉換為中文數字\n",
    "data[\"輸入\"] = data[\"輸入\"].apply(convert_numbers_to_chinese)\n",
    "# 資料預處理（假設已有資料X和標籤y）\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(data)):\n",
    "    X.append(jieba.lcut(data[\"輸入\"].iloc[i]))\n",
    "    y.append(data[\"分類\"].iloc[i])\n",
    "y = to_categorical(np.asarray(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text, word_index):\n",
    "    data = []\n",
    "    for sentence in text:\n",
    "        new_text = []\n",
    "        for word in sentence:\n",
    "            if word in word_index:\n",
    "                try:\n",
    "                    new_text.append(word_index[word])\n",
    "                except:\n",
    "                    new_text.append(0)\n",
    "        data.append(new_text)\n",
    "    data_lengths = [len(sentence) for sentence in data]\n",
    "    max_length = max(data_lengths)\n",
    "    \n",
    "    texts = pad_sequences(data, maxlen=7)\n",
    "    return texts\n",
    "X = tokenizer(X, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 切割訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train = X\n",
    "# y_train = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 3s 75ms/step - loss: 4.8353 - accuracy: 0.5357 - val_loss: 4.0310 - val_accuracy: 0.5634 - lr: 0.0050\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 3.0742 - accuracy: 0.6357 - val_loss: 2.9243 - val_accuracy: 0.6197 - lr: 0.0050\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 2.1615 - accuracy: 0.6500 - val_loss: 2.3683 - val_accuracy: 0.5775 - lr: 0.0050\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 1.7041 - accuracy: 0.6750 - val_loss: 2.1483 - val_accuracy: 0.5211 - lr: 0.0050\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 1.5351 - accuracy: 0.6143 - val_loss: 1.9863 - val_accuracy: 0.5493 - lr: 0.0050\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 1.3561 - accuracy: 0.6929 - val_loss: 1.8462 - val_accuracy: 0.5352 - lr: 0.0050\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.2861 - accuracy: 0.6607 - val_loss: 1.7656 - val_accuracy: 0.5775 - lr: 0.0050\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 1.2022 - accuracy: 0.6750 - val_loss: 1.7832 - val_accuracy: 0.4366 - lr: 0.0050\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.2288 - accuracy: 0.6286 - val_loss: 1.6310 - val_accuracy: 0.6056 - lr: 0.0050\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.1951 - accuracy: 0.6536 - val_loss: 1.7067 - val_accuracy: 0.5070 - lr: 0.0050\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.1415 - accuracy: 0.6571 - val_loss: 1.6362 - val_accuracy: 0.5775 - lr: 0.0050\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 1.1067 - accuracy: 0.6786 - val_loss: 1.5932 - val_accuracy: 0.5070 - lr: 0.0050\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 1.0448 - accuracy: 0.6893 - val_loss: 1.5131 - val_accuracy: 0.5775 - lr: 0.0050\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.0382 - accuracy: 0.6929 - val_loss: 1.5661 - val_accuracy: 0.5211 - lr: 0.0050\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.0496 - accuracy: 0.6786 - val_loss: 1.5104 - val_accuracy: 0.5493 - lr: 0.0050\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.0881 - accuracy: 0.6679 - val_loss: 1.4959 - val_accuracy: 0.5493 - lr: 0.0050\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 1.1269 - accuracy: 0.6500 - val_loss: 1.4743 - val_accuracy: 0.6056 - lr: 0.0050\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.1441 - accuracy: 0.6179 - val_loss: 1.5812 - val_accuracy: 0.5070 - lr: 0.0050\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.0630 - accuracy: 0.7000 - val_loss: 1.4145 - val_accuracy: 0.5775 - lr: 0.0050\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.0251 - accuracy: 0.6714 - val_loss: 1.4342 - val_accuracy: 0.5915 - lr: 0.0050\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 1.0497 - accuracy: 0.6821 - val_loss: 1.4548 - val_accuracy: 0.5493 - lr: 0.0050\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 1.0216 - accuracy: 0.6750 - val_loss: 1.4337 - val_accuracy: 0.5634 - lr: 0.0050\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.9972 - accuracy: 0.6929 - val_loss: 1.4402 - val_accuracy: 0.5634 - lr: 0.0050\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 1.0063 - accuracy: 0.6929 - val_loss: 1.3484 - val_accuracy: 0.5634 - lr: 0.0050\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.0080 - accuracy: 0.7000 - val_loss: 1.4864 - val_accuracy: 0.5211 - lr: 0.0050\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.9532 - accuracy: 0.7357 - val_loss: 1.3249 - val_accuracy: 0.6197 - lr: 0.0050\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.9574 - accuracy: 0.7107 - val_loss: 1.3676 - val_accuracy: 0.5915 - lr: 0.0050\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.9534 - accuracy: 0.7071 - val_loss: 1.4176 - val_accuracy: 0.5915 - lr: 0.0050\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.9946 - accuracy: 0.7143 - val_loss: 1.3515 - val_accuracy: 0.5775 - lr: 0.0050\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.9777 - accuracy: 0.6893 - val_loss: 1.3185 - val_accuracy: 0.6338 - lr: 0.0050\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.0406 - accuracy: 0.6500 - val_loss: 1.2953 - val_accuracy: 0.6338 - lr: 0.0050\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 1.0058 - accuracy: 0.6964 - val_loss: 1.3508 - val_accuracy: 0.5352 - lr: 0.0050\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.9602 - accuracy: 0.7000 - val_loss: 1.2624 - val_accuracy: 0.6056 - lr: 0.0050\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.9388 - accuracy: 0.7214 - val_loss: 1.3342 - val_accuracy: 0.5634 - lr: 0.0050\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.9366 - accuracy: 0.7429 - val_loss: 1.6539 - val_accuracy: 0.5493 - lr: 0.0050\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.9707 - accuracy: 0.7143 - val_loss: 1.3187 - val_accuracy: 0.5775 - lr: 0.0050\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.9372 - accuracy: 0.6929 - val_loss: 1.3371 - val_accuracy: 0.5775 - lr: 0.0050\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.8921 - accuracy: 0.7464 - val_loss: 1.2557 - val_accuracy: 0.6197 - lr: 0.0050\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.9131 - accuracy: 0.7143 - val_loss: 1.4443 - val_accuracy: 0.4789 - lr: 0.0050\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.9473 - accuracy: 0.7321 - val_loss: 1.3875 - val_accuracy: 0.5775 - lr: 0.0050\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.9126 - accuracy: 0.7643 - val_loss: 1.4194 - val_accuracy: 0.4507 - lr: 0.0050\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.9641 - accuracy: 0.7214 - val_loss: 1.4384 - val_accuracy: 0.5634 - lr: 0.0050\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.9828 - accuracy: 0.6750 - val_loss: 1.2906 - val_accuracy: 0.6056 - lr: 0.0050\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.8710 - accuracy: 0.7464 - val_loss: 1.2895 - val_accuracy: 0.6197 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.8575 - accuracy: 0.7536 - val_loss: 1.2938 - val_accuracy: 0.6056 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.8198 - accuracy: 0.7750 - val_loss: 1.2900 - val_accuracy: 0.5915 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.8106 - accuracy: 0.7714 - val_loss: 1.2860 - val_accuracy: 0.6056 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7990 - accuracy: 0.7821 - val_loss: 1.2762 - val_accuracy: 0.6056 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.8058 - accuracy: 0.7571 - val_loss: 1.2778 - val_accuracy: 0.5915 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.7636 - accuracy: 0.7857 - val_loss: 1.2782 - val_accuracy: 0.5915 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7701 - accuracy: 0.7786 - val_loss: 1.2787 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7477 - accuracy: 0.7893 - val_loss: 1.2797 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7418 - accuracy: 0.8000 - val_loss: 1.2814 - val_accuracy: 0.5634 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7846 - accuracy: 0.7679 - val_loss: 1.2820 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7465 - accuracy: 0.8036 - val_loss: 1.2823 - val_accuracy: 0.5634 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7455 - accuracy: 0.8000 - val_loss: 1.2818 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7493 - accuracy: 0.8107 - val_loss: 1.2840 - val_accuracy: 0.5915 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.7659 - accuracy: 0.7857 - val_loss: 1.2852 - val_accuracy: 0.5915 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7253 - accuracy: 0.8000 - val_loss: 1.2894 - val_accuracy: 0.5915 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7420 - accuracy: 0.8000 - val_loss: 1.2926 - val_accuracy: 0.5915 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7391 - accuracy: 0.7857 - val_loss: 1.2948 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7357 - accuracy: 0.8000 - val_loss: 1.2980 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7278 - accuracy: 0.7929 - val_loss: 1.2999 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7201 - accuracy: 0.8214 - val_loss: 1.3048 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7307 - accuracy: 0.8036 - val_loss: 1.3067 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.7255 - accuracy: 0.8071 - val_loss: 1.3054 - val_accuracy: 0.5634 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7282 - accuracy: 0.8000 - val_loss: 1.3102 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7192 - accuracy: 0.7821 - val_loss: 1.3127 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7276 - accuracy: 0.8000 - val_loss: 1.3179 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.7354 - accuracy: 0.8036 - val_loss: 1.3233 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.6934 - accuracy: 0.8214 - val_loss: 1.3273 - val_accuracy: 0.5634 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7177 - accuracy: 0.8036 - val_loss: 1.3277 - val_accuracy: 0.5775 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7233 - accuracy: 0.7821 - val_loss: 1.3292 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7192 - accuracy: 0.8071 - val_loss: 1.3287 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7190 - accuracy: 0.7893 - val_loss: 1.3322 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.7149 - accuracy: 0.8107 - val_loss: 1.3402 - val_accuracy: 0.5634 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7223 - accuracy: 0.8214 - val_loss: 1.3449 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.6824 - accuracy: 0.8357 - val_loss: 1.3473 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7027 - accuracy: 0.8036 - val_loss: 1.3491 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 0.7191 - accuracy: 0.7964 - val_loss: 1.3502 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7022 - accuracy: 0.8036 - val_loss: 1.3506 - val_accuracy: 0.5352 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7160 - accuracy: 0.7714 - val_loss: 1.3564 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.6974 - accuracy: 0.8179 - val_loss: 1.3520 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7194 - accuracy: 0.7857 - val_loss: 1.3501 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.6982 - accuracy: 0.8143 - val_loss: 1.3488 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.7061 - accuracy: 0.8071 - val_loss: 1.3537 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.6749 - accuracy: 0.8286 - val_loss: 1.3547 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.6835 - accuracy: 0.8250 - val_loss: 1.3635 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7100 - accuracy: 0.8250 - val_loss: 1.3580 - val_accuracy: 0.5352 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.6928 - accuracy: 0.8286 - val_loss: 1.3622 - val_accuracy: 0.5352 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.6928 - accuracy: 0.8000 - val_loss: 1.3566 - val_accuracy: 0.5352 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.6829 - accuracy: 0.8107 - val_loss: 1.3539 - val_accuracy: 0.5352 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.6789 - accuracy: 0.8214 - val_loss: 1.3518 - val_accuracy: 0.5211 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.6739 - accuracy: 0.8214 - val_loss: 1.3518 - val_accuracy: 0.5352 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.6676 - accuracy: 0.8357 - val_loss: 1.3505 - val_accuracy: 0.5352 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 0.7088 - accuracy: 0.8036 - val_loss: 1.3551 - val_accuracy: 0.5493 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.6944 - accuracy: 0.8214 - val_loss: 1.3551 - val_accuracy: 0.5211 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 0s 18ms/step - loss: 0.6685 - accuracy: 0.8464 - val_loss: 1.3557 - val_accuracy: 0.5211 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.6846 - accuracy: 0.8107 - val_loss: 1.3624 - val_accuracy: 0.5352 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 0.6626 - accuracy: 0.8214 - val_loss: 1.3706 - val_accuracy: 0.5493 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'LSTMmodel_20231226.keras'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential, save_model\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, BatchNormalization, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.constraints import max_norm\n",
    "from keras.initializers import he_normal\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from datetime import datetime\n",
    "# 創建模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(embedding_matrix), output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix], input_length=X.shape[1], \n",
    "                    trainable=False, embeddings_initializer=he_normal()))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, \n",
    "                             kernel_constraint=max_norm(5), kernel_regularizer=l2(0.01))))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(32, kernel_constraint=max_norm(5), kernel_regularizer=l2(0.01))))\n",
    "model.add(Dropout(0.3))  # 添加 Dropout 層，可根據需要調整 dropout 比例\n",
    "model.add(Dense(6, activation='softmax', kernel_regularizer=l2(0.01)))  # 在全連接層上應用 L2 正規化\n",
    "\n",
    "# 編譯模型，使用學習率調整器\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.005), metrics=['accuracy'])\n",
    "\n",
    "# 訓練模型，加入 callbacks\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[reduce_lr])\n",
    "\n",
    "# Generate today's date in the format YYYYMMDD\n",
    "today_date = datetime.today().strftime('%Y%m%d')\n",
    "\n",
    "# Construct the filename using the specified naming convention\n",
    "filename = f'LSTMmodel_{today_date}.keras'\n",
    "\n",
    "# Save the entire model in the .keras format with the constructed filename\n",
    "save_model(model, filename)\n",
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練(early stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "model.add(Embedding(input_dim=len(embedding_matrix), output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix], input_length=X.shape[1], \n",
    "                    trainable=False, embeddings_initializer=he_normal()))\n",
    "\n",
    "# Add a Bidirectional LSTM layer with 64 units\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, \n",
    "                             kernel_constraint=max_norm(5), kernel_regularizer=l2(0.01))))\n",
    "\n",
    "# Add Batch Normalization\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add another Bidirectional LSTM layer with 32 units\n",
    "model.add(Bidirectional(LSTM(32, kernel_constraint=max_norm(5), kernel_regularizer=l2(0.01))))\n",
    "\n",
    "# Add Dropout layer with a dropout rate of 0.3\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Add a Dense layer with 6 units and softmax activation function\n",
    "model.add(Dense(6, activation='softmax', kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Compile the model with categorical crossentropy loss and Adam optimizer with specified learning rate\n",
    "# Also, include accuracy as a metric\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Add EarlyStopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the training data and validate on the test data\n",
    "# Include the ReduceLROnPlateau and EarlyStopping callbacks\n",
    "# model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[reduce_lr, early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均數: 7.0\n",
      "中位數: 7.0\n"
     ]
    }
   ],
   "source": [
    "# 假設X是你的文本數據，已經通過tokenizer轉換為數字序列\n",
    "\n",
    "# 計算每個句子的長度\n",
    "sentence_lengths = [len(sentence) for sentence in X]\n",
    "\n",
    "# 計算平均數\n",
    "average_length = sum(sentence_lengths) / len(sentence_lengths)\n",
    "\n",
    "# 計算中位數\n",
    "sorted_lengths = sorted(sentence_lengths)\n",
    "mid = len(sorted_lengths) // 2\n",
    "median_length = (sorted_lengths[mid] + sorted_lengths[~mid]) / 2  # 使用~mid取得中位數，即使數量為奇數也可正確計算\n",
    "\n",
    "# 打印結果\n",
    "print(f\"平均數: {average_length}\")\n",
    "print(f\"中位數: {median_length}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
